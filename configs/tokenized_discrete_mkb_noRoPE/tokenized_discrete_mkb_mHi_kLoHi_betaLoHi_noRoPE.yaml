dataset_params:
  beta:
  - 0
  - 1
  - 3
  - 4
  dt: 0.1
  k:
  - 10
  - 11
  - 12
  - 13
  - 17
  - 18
  - 19
  - 20
  k_context: false
  m:
  - 6
  - 7
  - 8
  - 9
  - 10
  min_amplitude: 0.0
  min_seq_length: 20
  pin_amplitude: null
  seq_len: 100
  vary_length: false
  xv: false
model_name: tokenized_discrete_mkb_mHi_kLoHi_betaLoHi_noRoPE
model_params:
  bias: false
  block_size: 1024
  context_dim: null
  dropout: 0.0
  input_dim: 2
  n_embd: 64
  n_head: 8
  n_layer: 8
  tokenized: true
  use_pe: false
  use_rope: false
  vocab_size: 128
opt_params:
  beta1: 0.9
  beta2: 0.95
  decay_lr: true
  grad_clip: 1.0
  lr: 0.0005
  lr_decay_iter_frac: 1.0
  min_lr: 1.0e-06
  warmup_iter_frac: 0.05
  weight_decay: 0.01
parsing_params:
  beta_tuple: false
  k_tuple: false
  m_tuple: false
training_params:
  bs: 128
  bs_val: 1024
  num_train_iters: 20000
  num_val_seqs: 10000
  range_limit_tok: 2
  save_every: 1000
  val_every: 1000
